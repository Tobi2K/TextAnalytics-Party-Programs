{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# topic modeling playground\n",
    "In this notebook, different concepts for topic modeling will be tested and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/supelir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2022-02-01 15:06:15.854162: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-01 15:06:15.854227: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# preprocessing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "\n",
    "import tomotopy\n",
    "\n",
    "# Plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Party(Enum):\n",
    "    AFD = 0\n",
    "    CDU = 1\n",
    "    FDP = 2\n",
    "    GRUENE = 3\n",
    "    LINKE = 4\n",
    "    SPD = 5\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HDP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_md')\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# stopwords\n",
    "nltk_stopwords = stopwords.words('german')\n",
    "\n",
    "# build stopwords list\n",
    "all_stopwords = list(set(STOP_WORDS) | set(nltk_stopwords))\n",
    "with open('custom_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    all_stopwords += [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load files\n",
    "party_text = {}\n",
    "for party in Party:\n",
    "    all_stopwords.extend(['{}'.format(party.name.lower())])\n",
    "    with open('resources/' + party.name + '.txt', encoding='utf-8', errors='ignore') as txt:\n",
    "        file = \" \".join(l for l in txt)\n",
    "        # remove gender *\n",
    "        file = re.sub(r'\\*innen(\\w*)\\s', r'\\1 ', file)\n",
    "    party_text[party] = file\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for party in Party:\n",
    "    party_text[party] = [token.lemma_ for token in nlp(party_text[party]) if not token.is_stop and\n",
    "               not token.is_punct and\n",
    "               not token.is_space and\n",
    "               token.pos_ != 'NUM' and\n",
    "               not token.is_upper]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs:1, Num Vocabs:758, Total Words:8385\n",
      "Removed Top words:  Freier Demokrat Deutschland fordern setzen stärken europäisch Mensch schaffen Land\n"
     ]
    }
   ],
   "source": [
    "mdl = tomotopy.HDPModel(tw=tomotopy.TermWeight.ONE, min_cf=5, rm_top=10, alpha=0.1, gamma=1, initial_k=10, seed=99999)\n",
    "\n",
    "mdl.add_doc(party_text[Party.FDP])\n",
    "\n",
    "mdl.train(0)\n",
    "mdl.burn_in = 500\n",
    "\n",
    "print('Num docs:{}, Num Vocabs:{}, Total Words:{}'.format(\n",
    "    len(mdl.docs), len(mdl.used_vocabs), mdl.num_words\n",
    "))\n",
    "print('Removed Top words: ', *mdl.removed_top_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0000, LL per word: -6.855\n",
      "Iteration: 0050, LL per word: -6.877\n",
      "Iteration: 0100, LL per word: -6.9\n",
      "Iteration: 0150, LL per word: -6.906\n",
      "Iteration: 0200, LL per word: -6.908\n",
      "Iteration: 0250, LL per word: -6.904\n",
      "Iteration: 0300, LL per word: -6.927\n",
      "Iteration: 0350, LL per word: -6.942\n",
      "Iteration: 0400, LL per word: -6.929\n",
      "Iteration: 0450, LL per word: -6.932\n",
      "Iteration: 0500, LL per word: -6.946\n",
      "Iteration: 0550, LL per word: -6.918\n",
      "Iteration: 0600, LL per word: -6.93\n",
      "Iteration: 0650, LL per word: -6.922\n",
      "Iteration: 0700, LL per word: -6.944\n",
      "Iteration: 0750, LL per word: -6.942\n",
      "Iteration: 0800, LL per word: -6.971\n",
      "Iteration: 0850, LL per word: -6.995\n",
      "Iteration: 0900, LL per word: -6.954\n",
      "Iteration: 0950, LL per word: -6.989\n",
      "Iteration: 1000, LL per word: -6.997\n",
      "Iteration: 1050, LL per word: -6.987\n",
      "Iteration: 1100, LL per word: -6.982\n",
      "Iteration: 1150, LL per word: -6.995\n",
      "Iteration: 1200, LL per word: -6.983\n",
      "Iteration: 1250, LL per word: -6.984\n",
      "Iteration: 1300, LL per word: -7.014\n",
      "Iteration: 1350, LL per word: -7.012\n",
      "Iteration: 1400, LL per word: -7.071\n",
      "Iteration: 1450, LL per word: -7.072\n",
      "Iteration: 1500, LL per word: -7.076\n",
      "Iteration: 1550, LL per word: -7.058\n",
      "Iteration: 1600, LL per word: -7.045\n",
      "Iteration: 1650, LL per word: -7.073\n",
      "Iteration: 1700, LL per word: -7.144\n",
      "Iteration: 1750, LL per word: -7.117\n",
      "Iteration: 1800, LL per word: -7.162\n",
      "Iteration: 1850, LL per word: -7.15\n",
      "Iteration: 1900, LL per word: -7.161\n",
      "Iteration: 1950, LL per word: -7.174\n",
      "Iteration: 2000, LL per word: -7.219\n",
      "Iteration: 2050, LL per word: -7.165\n",
      "Iteration: 2100, LL per word: -7.183\n",
      "Iteration: 2150, LL per word: -7.238\n",
      "Iteration: 2200, LL per word: -7.216\n",
      "Iteration: 2250, LL per word: -7.24\n",
      "Iteration: 2300, LL per word: -7.269\n",
      "Iteration: 2350, LL per word: -7.267\n",
      "Iteration: 2400, LL per word: -7.303\n",
      "Iteration: 2450, LL per word: -7.297\n",
      "Iteration: 2500, LL per word: -7.305\n",
      "Iteration: 2550, LL per word: -7.276\n",
      "Iteration: 2600, LL per word: -7.288\n",
      "Iteration: 2650, LL per word: -7.31\n",
      "Iteration: 2700, LL per word: -7.313\n",
      "Iteration: 2750, LL per word: -7.305\n",
      "Iteration: 2800, LL per word: -7.297\n",
      "Iteration: 2850, LL per word: -7.244\n",
      "Iteration: 2900, LL per word: -7.26\n",
      "Iteration: 2950, LL per word: -7.322\n",
      "Iteration: 3000, LL per word: -7.339\n",
      "Iteration: 3050, LL per word: -7.273\n",
      "Iteration: 3100, LL per word: -7.347\n",
      "Iteration: 3150, LL per word: -7.354\n",
      "Iteration: 3200, LL per word: -7.371\n",
      "Iteration: 3250, LL per word: -7.355\n",
      "Iteration: 3300, LL per word: -7.388\n",
      "Iteration: 3350, LL per word: -7.391\n",
      "Iteration: 3400, LL per word: -7.329\n",
      "Iteration: 3450, LL per word: -7.327\n",
      "Iteration: 3500, LL per word: -7.381\n",
      "Iteration: 3550, LL per word: -7.345\n",
      "Iteration: 3600, LL per word: -7.398\n",
      "Iteration: 3650, LL per word: -7.42\n",
      "Iteration: 3700, LL per word: -7.396\n",
      "Iteration: 3750, LL per word: -7.38\n",
      "Iteration: 3800, LL per word: -7.404\n",
      "Iteration: 3850, LL per word: -7.418\n",
      "Iteration: 3900, LL per word: -7.429\n",
      "Iteration: 3950, LL per word: -7.382\n",
      "Iteration: 4000, LL per word: -7.37\n",
      "Iteration: 4050, LL per word: -7.408\n",
      "Iteration: 4100, LL per word: -7.412\n",
      "Iteration: 4150, LL per word: -7.398\n",
      "Iteration: 4200, LL per word: -7.414\n",
      "Iteration: 4250, LL per word: -7.403\n",
      "Iteration: 4300, LL per word: -7.423\n",
      "Iteration: 4350, LL per word: -7.42\n",
      "Iteration: 4400, LL per word: -7.437\n",
      "Iteration: 4450, LL per word: -7.448\n",
      "Iteration: 4500, LL per word: -7.413\n",
      "Iteration: 4550, LL per word: -7.444\n",
      "Iteration: 4600, LL per word: -7.446\n",
      "Iteration: 4650, LL per word: -7.46\n",
      "Iteration: 4700, LL per word: -7.397\n",
      "Iteration: 4750, LL per word: -7.513\n",
      "Iteration: 4800, LL per word: -7.495\n",
      "Iteration: 4850, LL per word: -7.422\n",
      "Iteration: 4900, LL per word: -7.481\n",
      "Iteration: 4950, LL per word: -7.542\n",
      "Iteration: 5000, LL per word: -7.537\n",
      "<Basic Info>\n",
      "| HDPModel (current version: 0.12.2)\n",
      "| 1 docs, 8385 words\n",
      "| Total Vocabs: 6463, Used Vocabs: 758\n",
      "| Entropy of words: 6.41334\n",
      "| Entropy of term-weighted words: 6.41334\n",
      "| Removed Vocabs: Freier Demokrat Deutschland fordern setzen stärken europäisch Mensch schaffen Land\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 5000, Burn-in steps: 500\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -7.53733\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 5 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 10 (the number of top words to be removed)\n",
      "| initial_k: 10 (the initial number of topics between 2 ~ 32767 The number of topics will be adjusted for data during training)\n",
      "| alpha: 0.1 (concentration coeficient of Dirichlet Process for document-table )\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| gamma: 1.0 (concentration coeficient of Dirichlet Process for table-topic)\n",
      "| seed: 99999 (random seed)\n",
      "| trained in version 0.12.2\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (concentration coeficient of Dirichlet Process for document-table)\n",
      "|  3.9901\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "| gamma (concentration coeficient of Dirichlet Process for table-topic)\n",
      "|  39.472\n",
      "| Number of Topics: 30\n",
      "| Number of Tables: 30\n",
      "|\n",
      "<Topics>\n",
      "| #0 (247) : Kind schnellen gemeinsam Ausbau Ausbildung\n",
      "| #1 (180) : sorgen ermöglichen erreichen regelmäßig Hilfe\n",
      "| #2 (250) : gelten Einsatz Digitalisierung Selbstständige bringen\n",
      "| #3 (242) : stellen Zugang Datum Mobilität rechtlich\n",
      "| #4 (270) : Entwicklung Frau einfahren Regel Vorbild\n",
      "| #5 (249) : digitale unabhängig Leben Politik Rahmenbedingungen\n",
      "| #6 (315) : mein Bildung entlasten ausbauen Chance\n",
      "| #7 (266) : unterstützen Zudem bieten Reform einfach\n",
      "| #8 (305) : brauchen stehen Standard abbauen Familie\n",
      "| #9 (326) : Bund Förderung Maßnahme Nutzung Fall\n",
      "| #10 (260) : digital sozial erhöhen Energie Grundlage\n",
      "| #11 (310) : Wettbewerb nutzen Zukunft Form langfristig\n",
      "| #12 (162) : hoch Schritt Gesundheitswesen Lernen alt\n",
      "| #13 (281) : Schutz Innovation gut Arbeit verbessern\n",
      "| #14 (387) : Ziel bleiben wirtschaftlich Selbstbestimmung China\n",
      "| #15 (340) : fördern verhindern wichtig erhalten Verwaltung\n",
      "| #16 (238) : Chance öffentlich Pflege Weg Entwicklungspolitik\n",
      "| #17 (294) : Herausforderung weiterentwickel weltweit Infrastruktur Entscheidung\n",
      "| #18 (271) : Beitrag Menschenrechte Angebot Kompetenz Hand\n",
      "| #19 (367) : deutsch Möglichkeit beruflich Unternehmen Investition\n",
      "| #20 (301) : Freiheit privat neu zudem erleichtern\n",
      "| #21 (250) : schützen Prozent Wirtschaft erfolgen einzeln\n",
      "| #22 (264) : nachhaltig abschaffen europäische bekämpfen tragen\n",
      "| #23 (247) : Schule einheitlich Rahmen Forschung vereinfachen\n",
      "| #24 (330) : Bürger modern lehnen Gesellschaft staatlich\n",
      "| #25 (224) : gesetzlich politisch verpflichten Aufgabe Einrichtung\n",
      "| #26 (418) : Staat ermöglichen stark insbesondere gewährleisten\n",
      "| #27 (136) : Voraussetzung Modernisierung umfassend Stelle Entwicklung\n",
      "| #28 (301) : Europa international Hochschule Person Weiterbildung\n",
      "| #29 (354) : Unternehmen Bürgerin Demokratie Sicherheit Regelung\n",
      "|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supelir/.local/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/supelir/.local/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5000, 50):\n",
    "    print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word))\n",
    "    mdl.train(50)\n",
    "print('Iteration: {:04}, LL per word: {:.4}'.format(mdl.global_step, mdl.ll_per_word))\n",
    "\n",
    "mdl.summary()\n",
    "\n",
    "live_topics = [k for k in range(mdl.k) if mdl.is_live_topic(k)]\n",
    "\n",
    "topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])\n",
    "topic_term_dists = topic_term_dists[live_topics]\n",
    "topic_term_dists /= topic_term_dists.sum(axis=1, keepdims=True)\n",
    "\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
    "doc_topic_dists = doc_topic_dists[:, live_topics]\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "\n",
    "doc_lengths = np.array([len(doc.words) for doc in mdl.docs])\n",
    "vocab = list(mdl.used_vocabs)\n",
    "term_frequency = mdl.used_vocab_freq\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists,\n",
    "    doc_topic_dists,\n",
    "    doc_lengths,\n",
    "    vocab,\n",
    "    term_frequency,\n",
    "    start_index=0,\n",
    "    sort_topics=False\n",
    ")\n",
    "pyLDAvis.save_html(prepared_data, 'ldavis.html')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}