\section{Related Works}\label{sec:related_works}
Many applications in natural language processing are based on topic models. In addition, various approaches of topic modeling in subjects like social network, software engineering, linguistic science and more have been published. 

Whereas LDA model extracts latent topics by being integrated for measuring probabilities over a collection of documents. In NLP it facilitates approaches in different sectors like sentiment analysis or semantic knowledge inference in online news media \cite{6033626}, and much more. Pretrained BERT models enable bidirectional representations unlike recent language representation, typically left-to-right language models \cite{DevlinPres}. Its variable fine-tuning capability allows more difficult approaches like multilingual argument mining \cite{toledo2020multilingual} or detect fake news based on social bot activities like during the COVID-19 Pandemic \cite{9666618}.
The works in \citet{zirn2014multidimensional} and \citet{koh2021predicting} focus on social science with Latent-Dirichlet Allocation (LDA) models and BERT models respectively, as used in this work. 

\citet{zirn2014multidimensional} is an extension of the works from \citet{stuckenschmidt2012multi} and based on its approaches. In it, the authors present a multidimensional thematic analysis of political texts, political manifestos and transcripts as well as speeches in Germany since 1990. Focusing on the manifestos and transcripts, this work uses two different models for evaluating the political texts which are then compared on the outcome. For data preparation, the text is split automatically via TextTilling into topically coherent sub-parts. Then the cosine similarity  between adjacent token-sentences is integrated which is later also used as scheme for measuring the following step. Finally, similarity-dependent change boundaries between token sentences are identified. Based on the standard LDA more detailed in \citet{newman2009distributed,blei2003latent} and will follow in Section 3, the two different models used are Labeled LDA and Logic LDA.
While for the labeled LDA a relation between topics and labels is created via Bernoulli coin-toss in a heuristic way, the logic LDA combines word-document statistics like in standard LDA and domain knowledge
rules as in Markov Logic Networks.
The results from the two variations, a governing party which has got more similarity to the ministries topic and who gets the actual seat within parties program similarity, are compared with a baseline and a majority baseline for specifying a assignment of party closeness and given ministry position. Finally overall parties manifesto similarity in percentage is determined.
%Baseline majority is based on the acceptance that the party with the higher share has got all ministries which makes up the first evaluation in the paper, comparing overall governing parties texts similarity including seed words measuring to the ministry sector as a topic an actual given a party member the ministry position. Second is the overall parties manifesto similarity in percentage of 5 parties.
\\
The second work, \citet{koh2021predicting}, introduces different BERT models and applies those on political manifestos. Oriented on the approaches from \citet{abercrombie2019policy} the F1-score  is integrated for weighting performance of different text analysing models on political texts \cite{zhang2015estimating}. \citet{koh2021predicting} use the state-of-the-art BERT models with bidirectional gated recurrent units (GRU) models as well as deep convolutional neural network (CNN) models. They were also compared to other models like Multinomial Naive Bayes or support vector machines (SVM). The outcomes were that there is no significant wide-range difference between those techniques over low epoch rate. If it comes to increasing following generations the deep convolutional model shows best performance. Before training, data preparation is done by stemming and lemmatizing party manifestos and subsequently splitting them into quasi-sentences which are individual statements, not spanning more than one grammatical sentence.
The used principle of the BERT-CNN model here is based on the BERT BASE tokenizer, in that it uses a masked language model which allows unidirectional constraints to overcome restrictions. Filtering with convolutional filters, results in a given document setting a rectified linear activation function. Next, a 1D-max pooling layer and  a dropout layer for preventing overfitting during training is integrated. In summary, neural networks performance over policy domains yield the best approach.
%Finally, performance rates, including different policy domains and preferences, are calculated. It turns out that including a deep learning progress with a neural network is the best in overall performance regarding accuracy and loss over increasing generations or epochs of training on topics with a given data set.